{
  "cells": [
    {
      "metadata": {
        "cell_id": "00000-0dffe957-96c3-40a0-9751-73d4a3d3dfa4",
        "deepnote_cell_height": 133.953125,
        "deepnote_cell_type": "markdown"
      },
      "cell_type": "markdown",
      "source": "# 9.8: Samples Vs. Reality:\n## The Story of the Sampling Distribution"
    },
    {
      "metadata": {
        "cell_id": "00001-10b81647-cd83-4c52-9111-2a12746f1bd7",
        "deepnote_cell_height": 206.953125,
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1971,
        "execution_start": 1629938459607,
        "output_cleared": false,
        "source_hash": "54687bae",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This code will load the R packages we will use\nsuppressPackageStartupMessages({\n    library(coursekata)\n})\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "cell_id": "00003-43fda0d1-ea7a-45f2-bacd-ac2c70aa1720",
        "deepnote_cell_height": 69.96875,
        "deepnote_cell_type": "markdown"
      },
      "cell_type": "markdown",
      "source": "## 0.0 - The Tipping Experiment\nThe `TipExperiment` data comes from a study carried out by a group of researchers and published in a [1996 paper](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1559-1816.1996.tb01847.x). In this experiment, 44 restaurant tables were randomly assigned to either receive smiley faces on their checks or not. The researchers hypothesized that tables would tip a higher percentage of the check if they got a hand-drawn smiley face on the check than if they did not.\n\nHere is a sample of 6 of the tables in the data frame."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# run this code block to look at the data from the tip experiment\nhead(TipExperiment)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "0.1 - What does each row represent?  What do each of the variables (columns) mean? How can you identify the tables that got a smiley face?  What about the tables that didn't get a smiley face?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##  1.0 Modeling the Effect of Smiley Faces"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "1.1 - Write the word equation (`Outcome` = `Explanatory` + other stuff) for the hypothesis that smiley faces increase the tip amount."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "1.2 - Complete the code below to create a visualization of this hypothesis"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# complete the jitter plot\ngf_jitter( OUTCOME ~ EXPLANATORY, data = DataSet, width = .1)  %>%\n  gf_model(Tip ~ Condition, color = \"red\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "1.3 - What does the display tell us?  What does each dot represent?  How do we decide where to put a dot in this display? \n\nBased on the display, do you beleive the hypothesis is true? why?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "1.4 - How big was the difference between the groups of tables (those who got smiley faces and those who did not)?  What does that tell you about the hypothesis?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2.0 The Empty Model\n\nUnfortunately, even in a perfectly done experiment, there are two possible reasons why the smiley face group might have given bigger tips on average. \n- The first and most interesting possible reason is that there is a causal relationship between drawing a smiley face and tips! \n\nThat would be cool if a little drawing really does make people tip more generously. But, there is a second reason for any observed difference between the groups: \n- Random sampling variation. If we just randomly assigned tables to one of two groups and did not do anything special to the two groups (no tables get smiley faces), we would still expect some difference in tips across the two groups just by ***chance***."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "2.1 - What would the word equation be for the hypothesis that smiley faces have no effect on tips?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "2.2 - What would we expect the difference between groups to be if smiley faces had no effect on tips?  Would that always be the case?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3.0 The Roll of Randomness\n\nThe differences in distributions between the control condition and the smiley face condition could be due to either:\n- the causal effect of smiley faces, or \n- randomness, or \n- a combination of the two. \n\nHow can we tell which it is? One tool that can help here is simulation, using R, and particularly, the simulation of randomness. Let’s explore this tool, and see how it could help us."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It’s easy to understand the hypothesis that maybe smiley faces caused tables to tip slightly higher percentages of the check. But how could randomness cause the smiley face group to tip larger percentages of their checks?\n\n3.1 - Imagine that we took the actual tip percentages from the 44 tables in the `TipExperiment` data frame and just shuffled them up into two random groups. Would we be able to see the effect (if any) that smiley faces have on tips?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "3.2 - If we did this shuffling action, is it possible that one of the two groups would look like they tipped more than the other group?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Using R, we don’t have to just wonder about randomness as a data generating process. We can actually simulate it. There is a function called `shuffle()` that we can use to randomly shuffle the 44 `Tip` percentages from the data frame into the two conditions.\n\nTo see how this works, let’s look at 6 tables from the `TipExperiment` data frame (see table below). On the left, we see that the three tables in the control condition tipped 39, 34, and 31 percent, respectively. Notice that although there is overlap in tipping behavior between the two groups, the smiley face group seems to have tipped a bit more.\n\n<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vRQ6a1fHWCkoNN0_H2xJrG18rsKGCp8DLIRo1vxN9lNtqIQaMBlfnlakJtMn9BKwra8NZJS0832jvzD/pub?w=800\" alt=\"Table comparing origional tips to shuffled tips\" title=\"Shuffled Tips\" />\n\nOn the right, the percentages are shuffled, though everything else remains the same. The table IDs are still in the same order; and the first three tables are still in the control condition, the next three in the smiley face condition. The tips, though, have been randomly paired with tables. Notice that the tip percentages 39 and 34, which originally were in the control group, are now in the smiley face group.\n\n3.3 - After shuffling the tips, which group now seems to tip a bit more?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "***3.4 - Takeaway:*** For the *actual* data, differences between the two groups could either be due to\n- the smiley faces, or \n- randomness. \n\nBut in the case of the *shuffled* data, the only explination for the difference between the groups is \n- randomness\n\n...Even when the only cause is randomness, **the groups can still look different from one another!**  To see this in action, run the following code block"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# add the shuffle function to this code\nTipExperiment$ShuffTip <- TipExperiment$Tip\n\n# this makes a jitter plot of the shuffled data and adds means for both groups\ngf_jitter(ShuffTip ~ Condition, data = TipExperiment, width = .1) %>%\n  gf_labs(title = \"Shuffled Data\") %>%\n  gf_model(ShuffTip ~ Condition, color = \"orchid\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "nbsimplegrader": {
          "config_key": "markdown.view-only",
          "type": "markdown",
          "label": "Markdown: View-Only",
          "runnable": false,
          "editable": false,
          "respondable": false,
          "response": null
        }
      },
      "cell_type": "markdown",
      "source": "3.5 - As we can see, some shuffles produce distributions where the tips look similar across conditions. Other shuffles result in higher tips from the control group, while in others the smiley face tables appear to be tipping more.\n\nWhy do we see these different patterns of shuffled data?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4.0 How Shuffling Can Help Us Understand Real Data Better\n\nLet’s go back to the question we were asking before we started shuffling tips. Are the slight differences in tips related to adding smiley faces to checks due to the smiley faces, or could they be just due to randomness? Shuffling tips provides us with a way to begin answering this question.\n\nBy graphing multiple sets of randomly-generated results, we can look to see whether the pattern observed in the real data looks like it could be randomly generated, or if it looks markedly different from the randomly-generated patterns. If it looks markedly different, we might be more likely to believe that smiley faces had an effect. If it looks similar to the random results, we might be more inclined to believe that the effect, even if apparent in the data, could simply be the result of randomness.\n\nBelow we show nine different plots. Eight of them are the result of random shuffles of tips; the other one, in the upper left with red lines for averages, is the plot of the actual data. Take a look at all these plots, and compare the plot of real data to the other plots.\n\n<img src=\"https://coursekata-course-assets.s3.us-west-1.amazonaws.com/UCLATALL/czi-stats-course/rVNs1G7M.png\" alt=\"A9 graphs of tips by condition, one is the real data, the rest are from shuffled data\" title=\"Can you find me!?\" />"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "4.1 - Based on just looking at the graphs, do you think the pattern seen in the real data could have been generated by a purely random DGP (such as shuffle())? Explain your answer."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "4.2 - What would have to happen for you to be convinced that smiley faces were what caused the differences in tips amoung tables and not just coincidence?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 5.0 Which Hypothesis Is It?\n\nIn the origional graph we can see that the tables in the smiley face condition tipped more, on average, than those in the control condition. But we also can see a great deal of overlap between the two distributions. It’s possible that drawing the smiley face on the check caused tables to tip a bit more. But it’s also possible that the effect in the data is just the result of random sampling variation."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# here is the origioanl again, so you don't have to keep scrolling up.\ngf_jitter(Tip ~ Condition, data = TipExperiment, width = .1)  %>%\n  gf_model(Tip ~ Condition, color = \"red\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can actualy use R to tell us what the difference between the groups is.\n\n5.1 - Run the code below and see if you can identify what the numbers mean."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "lm(formula = Tip ~ Condition, data = TipExperiment)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To convince us the different between the groups is actually the smiley faces and not just random coincidence, the gap between the groups has to be *bigger* than the sort of gaps we would expect to see from random coincidence.\n\n5.1 - If there was no effect of smiley faces on tips, what kind of gaps would we expect to see?  Which of these would be beleivable?\n- 0\n- -2\n- 1\n- 6.045\n- 27\n- Something else?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Even though our data suggests the effect of smiley faces is a 6 percent increase in tips, is it possible that such an effect could have been produced by a *universal reality* in which the effect is actually equal to 0?  What are the types of gaps we would expect to see in our data if there was no smiley face effect?\n\n5.2 - To answer that question with more precission, we can have R shuffle our data many times, and report back how big the gap is.  run the following code and explain what it is doing."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# this code calculates the gap for the actual data\nb1(Tip ~ Condition, data = TipExperiment)\n\n# this code shuffles the Tip variable before calculating the gap\nb1(shuffle(Tip) ~ Condition, data = TipExperiment)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "5.3 - You can add the `do()` function to the code to tell R to run it more than once (so you don't have to keep pressing the run button).  Try modifying the code in the window below to produce 10 shuffles of the tips across conditions, and 10 randomized estimates of what gaps we would expect."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# modify to produce 10 shuffled gaps\ndo( ) * b1(shuffle(Tip) ~ Condition, data = TipExperiment)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The ten estimates that resulted when you ran the code will not be exactly the same as the estimates everyone else got (because its random). But notice that some of these shuffled gaps are positive and some are negative, some are quite different from 0 (e.g., 4.68 and -3.77) and some are fairly close to 0 (e.g., -0.13). Feel free to run the code a few times to get a sense of how these gaps can vary.\n\nIf we continue to generate random gaps, we would start to see that although they are rarely exactly 0, they are clustered around 0. This is because shuffle simulates the data we would get if the hypothesis that smiley faces don't change the tips was true, and in which the tables would have tipped the same amount even if they were put into the other condition.\n\n**Bear in mind: Each of these gaps results from a random process that has nothing to do with whether the table actually got smiley faces or not.** The `shuffle()` function mimics the hypothesis where the gap is 0. Sometimes we call this “the parent population” because when it generates gaps, these “children” tend to resemble their parent."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "5.4 - It is rather difficult to get a good sense of how big these gaps are ...and to check them all to see if they could possibly produce the 6.045 gap we saw in the actual data.  One way to visualize them more easily is with a histogram.  Run the code below to get a feel for what 1000 gaps look like:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# first we need to save the resulting gaps (don't forget to increase the number to 1000)\nsdob1 <- do(10)*b1(shuffle(Tip) ~ Condition, data = TipExperiment)\n\n#then we make a histogram showing all the gaps at once:\ngf_histogram(~b1, data = sdob1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Just for fun, add a line to show us where the actual gap was.  Do you think this gap could have come from this hypothsis? Why or why not?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "5.5 - Let's play devil's advocate for a moment and pretend that smiley faces don't matter, everyone tips the same (so the gap *should* be 0).  What are the chances of getting a gap as big as what we see in our actual data (or even bigger)?  Try running the code below to find out."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "#This code will count how many of the 1000 gaps were at least 6.045 or more\ntally(~b1 > 6.045, data = sdob1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So, what are the chances of this happeneing if smiley faces aren't actually making a difference?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 6.0 Conclussion\n\nWhat we did just now is more or less how every scientific study goes.  Everything we know about our universe boild down to doing this same thought process:\n- Start with your two beleifes: that something is going on, or that it isn't\n- Think about what data would look like if nothing was going on.  How would it be different from your beleif that something *is* going on?\n- Go out and gather data *carefully* so that you can see if that difference is there.\n- Rule out that the difference is a random coincidence by statistically measuring how likely that big of a difference would be to occur by random coincidence.\n- If that probability is very small, then it is likely because something is actually going on.  So we chose to believe that until we find evidence that something else is going on."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "6.1 - What is this method called?  Can you think of other examples where this method has been used to understand the universe we live in?"
    }
  ],
  "metadata": {
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "7ed061a2-5d00-4cb9-87e2-5a3d3b18878d",
    "kernelspec": {
      "name": "ir",
      "display_name": "R",
      "language": "R"
    },
    "language_info": {
      "name": "R",
      "codemirror_mode": "r",
      "pygments_lexer": "r",
      "mimetype": "text/x-r-source",
      "file_extension": ".r",
      "version": "4.3.3"
    },
    "nbsimplegrader": {
      "publish_config": {
        "classes": [],
        "tools": [],
        "options": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}